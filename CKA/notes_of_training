# This file records the kubernetes commands in CKA training course.

/************************************ Practice with pods ************************************/

1. Get running pods numbers
   kubectl get pods

2. Create a new pod with the nginx image
   kubectl run nginx --image=nginx      // kubectl run POD_NAME --image=POD_IMAGE_NAME
   
   kubectl run nginx --image=nginx --dry-run=client -o yaml > pod.yaml  // one can dry run to check pod details in yaml file
   kubectl apply -f pod.yaml
   
3. What is the image used to create the new pods?
   kubectl describe pod POD_NAMES | grep -i image
   
4. Which nodes are these pods placed on?
   kubectl get pods -o wide    // There is a column named 'node'
   
5. How many containers are part of the pod 'webapp'?
   kubectl get pods webapp
   
6. What is the state of container 'agentx' in the pod 'webapp'?
   kubectl describe pod webapp  // then to check state under each container
   
7. Delete pod webapp
   kubectl delete pod webapp
   
8. Change the image name for a pod
   kubectl edit pod nginx        // find the image name and edit it.


/************************************ Practice with ReplicaSets ************************************/

9. Get replicasets
   kubectl get replicasets.apps
   
controlplane ~ ✖ kubectl get replicasets.apps
NAME              DESIRED   CURRENT   READY   AGE
new-replica-set   4         4         0       36s

We can see that the DESIRED pods numbers in the replica. Replica will always make sure the DESIRED number of pods running. How many are deleted, then how many will be created automatically.

10. Get the image used to create pods in the replica.
    kubectl describe replicasets.apps RELICA_NAME
    
11. Correct replica apiVersion:
    controlplane ~ ➜  kubectl apply -f replicaset-definition-1.yaml 
error: unable to recognize "replicaset-definition-1.yaml": no matches for kind "ReplicaSet" in version "v1"

   -> apps/v1
   
12. Delete ReplicaSet
    kubectl delete replicasets.apps REPLICA_NAME
    
13. Scale (up/down) replica
    kubectl scale replicaset --replicas=5 REPLICA_SET_NAME    // up from 4 to 5
    kubectl scale replicaset --replicas=2 REPLICA_SET_NAME    // dwon from 5 to 2



/************************************ Practice with Deployment ************************************/


14. How many deployments
    kubectl get deployments.apps
    
15. Create deployment:
    Name: httpd-frontend;
Replicas: 3;
Image: httpd:2.4-alpine


    kubectl create deployment httpd-frontend --image=httpd:2.4-alpine
    
    kubectl scale deployment --replicas=3 httpd-frontend
    
    

/************************************ Practice with Namespace ************************************/

16.  How many Namespace
     kubectl get ns --no-headers | wc -l
     
17. How many pods exist in the namespace 'research'?
    kubectl get pods -n research --no-headers
    
18. Create a POD in the 'finance' namespace. (Name: redis, Image Name: redis)
    1) kubectl run redis --image=redis --dry-run=client -o yaml > pod.yaml
    2) vi pod.yaml
       apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: redis
  name: redis
  namespace: finance    #add this line 
spec:
  containers:
  - image: redis
    name: redis
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

    3) kubectl apply -f pod.yaml
    
19. Which namespace has the 'blue' pod in it?
    kubectl get pods --all-namespaces | grep blue
    
 
 /************************************ Practice with Service ************************************/
 
 20. How many Services exist on the system?
     kubectl get services
     
 21. What is the targetPort configured on the kubernetes service?
     controlplane ~ ✖ kubectl describe service kubernetes
Name:              kubernetes
Namespace:         default
Labels:            component=apiserver
                   provider=kubernetes
Annotations:       <none>
Selector:          <none>
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.43.0.1
IPs:               10.43.0.1
Port:              https  443/TCP
TargetPort:        6443/TCP                # Answer is 6443
Endpoints:         10.6.235.3:6443
Session Affinity:  None
Events:            <none>

22. How many Deployments exist on the system now?
    kubectl get deployments.apps
    
23. What is the image used to create the pods in the deployment?
    kubectl describe deployments.apps simple-webapp-deployment
    
24. Create a new service to access the web application using the service-definition-1.yaml file
Name: webapp-service
Type: NodePort
targetPort: 8080
port: 8080
nodePort: 30080
selector: simple-webapp

    kubectl expose deployment simple-webapp-deployment --name=webapp-service --target-port=8080 --type=NodePort --port=8080 --dry-run=client -o yaml > svc.yaml
    kubectl apply -f svc.yaml


 /************************************ Practice with Imperative Commands ************************************/
 
 25. Deploy a pod named nginx-pod using the nginx:alpine image. Use imperative commands only.
     kubectl run nginx-pod --image=nginx:alpine
     
     Deploy a redis pod using the redis:alpine image with the labels set to tier=db.
     kubectl run redis --image=redis:alpine --labels=tier=db
         
 26. Create a service redis-service to expose the redis application within the cluster on port 6379.
     kubectl expose pod redis --name redis-service --port 6379 --target-port 6379
     
 27. Create a deployment named webapp using the image kodekloud/webapp-color with 3 replicas.
     controlplane ~ ➜  kubectl create deployment webapp --image=kodekloud/webapp-color
     deployment.apps/webapp created

     controlplane ~ ➜  kubectl scale deployment --replicas=3 webapp
     deployment.apps/webapp scaled
     
28.  Create a new pod called custom-nginx using the nginx image and expose it on container port 8080.
     kubectl run custom-nginx --image=nginx --port 8080  
     
29. Create a new deployment called redis-deploy in the dev-ns namespace with the redis image. It should have 2 replicas.
     kubectl create deployment redis-deploy --image=redis --namespace=dev-ns --dry-run=client -o yaml > redis.yaml
     kubectl apply -f redis.yaml
     
30. Create a pod called httpd using the image httpd:alpine in the default namespace. Next, create a service of type ClusterIP by the same name (httpd). The target port for the service should be 80.
    kubectl run httpd --image=httpd:alpine --port 80 --expose --dry-run=client -o yaml
    
 
/************************************ Practice with Manually schedule ************************************/
31. Manually schedule the pod on node01. Delete and recreate the POD if necessary.
    kubectl delete pod nginx
    
    root@controlplane:~# vi nginx.yaml 
root@controlplane:~# cat nginx.yaml 
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  nodeName: node01     #add nodeName to pod yaml file
  containers:
  -  image: nginx
     name: nginx
     
     
     kubectl apply -f nginx.yaml
     
     
 
 /************************************ Practice with Labels and Selectors ************************************/    
 32. We have deployed a number of PODs. They are labelled with tier, env and bu. How many PODs exist in the dev environment?
     kubectl get pods -l env=dev
     
     How many PODs are in the finance business unit (bu)?
     kubectl get pods -l bu=finance
     
 33. How many objects are in the prod environment including PODs, ReplicaSets and any other objects?
     kubectl get all -l env=prod --no-headers | wc -l
     
     Identify the POD which is part of the prod environment, the finance BU and of frontend tier?
     kubectl get pods -l env=prod,bu=finance,tier=frontend
     
     
/************************************ Practice with Taint and Tolerance ************************************/    
34. Create a taint on node01 with key of spray, value of mortein and effect of NoSchedule
    kubectl taint node node01 spray=mortein:NoSchedule
   
35. Create another pod named bee with the nginx image, which has a toleration set to the taint mortein.
    kubectl run bee --image=nginx --restart=Never --dry-run=client -o yaml > bee.yaml
       
       cat bee.yaml
       apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: bee
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
 # Add below content
  tolerations:
          - effect: NoSchedule
            key: spray
            operator: Equal
            value: mortein
          
          kubectl apply -f bee.yaml
          
36. Remove the taint on controlplane, which currently has the taint effect of NoSchedule.
    kubectl taint node controlplane node-role.kubernetes.io/master:NoSchedule-    # add - 
    
 /************************************ Practice with Node Affinity ************************************/ 
 37. How many Labels exist on node node01?
     kubectl  get  nodes node01 --show-labels 
     
 38. Apply a label color=blue to node node01
     kubectl label nodes node01 color=blue
  
 39. Create a new deployment named blue with the nginx image and 3 replicas.
     kubectl create deployment blue --image=nginx
     kubectl scale deployment blue --replicas=3
    
 40. Set Node Affinity to the deployment to place the pods on node01 only.
     Name: blue
Replicas: 3
Image: nginx
NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution
Key: color
values: blue


      1) add affinity rule:
        spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: color
                operator: In
                values:
                  - blue
                  
       2) kubectl delete deployments.apps blue
       
       3) kubectl applfy -f blue.yaml
      
      
41. Create a new deployment named red with the nginx image and 2 replicas, and ensure it gets placed on the controlplane node only.
    Use the label - node-role.kubernetes.io/master - set on the controlplane node.      
Name: red
Replicas: 2
Image: nginx
NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution
Key: node-role.kubernetes.io/master
Use the right operator    

         affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/master
                operator: Exists
                
root@controlplane:~# kubectl create deployment red --image=nginx --dry-run=client -o yaml > red.yaml
root@controlplane:~# vi red.yaml 
root@controlplane:~# kubectl apply -f red.yaml 
                  
/************************************ Practice with Resource ************************************/    
42. The elephant pod runs a process that consume 15Mi of memory. Increase the limit of the elephant pod to 20Mi.
    controlplane ~ ➜  kubectl get pod elephant -o yaml > elephant.yaml
    controlplane ~ ➜  vi elephant.yaml    # modify the limit memory to 20Mi
    controlplane ~ ➜  kubectl delete pod elephant 
            pod "elephant" deleted
    controlplane ~ ➜  kubectl apply -f elephant.yaml 
    
    
/************************************ Practice with DaemonSets ************************************/ 
43. How many DaemonSets are created in the cluster in all namespaces?
    kubectl get daemonsets --all-namespaces
   
44. On how many nodes are the pods scheduled by the DaemonSets kube-proxy
    kubectl -n kube-system get pods -o wide | grep proxy

45. What is the image used by the POD deployed by the kube-flannel-ds DaemonSet?
    kubectl -n kube-system describe ds kube-flannel-ds | grep -i image
    
    
46. Deploy a DaemonSet for FluentD Logging. Use the given specifications.
Name: elasticsearch
Namespace: kube-system
Image: k8s.gcr.io/fluentd-elasticsearch:1.20

     kubectl create deployment elasticsearch --image=k8s.gcr.io/fluentd-elasticsearch:1.20 --dry-run=client -o yaml > elastic.yaml
     
     vi elastic.yaml and edit as below:
     apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: elasticsearch
  name: elasticsearch
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: elasticsearch
    spec:
      containers:
      - image: k8s.gcr.io/fluentd-elasticsearch:1.20
        name: fluentd-elasticsearch
        
        
        kubectl apply -f elastic.yaml 
        kubectl -n kube-system get ds elasticsearch

/************************************ Practice with Static pods ************************************/ 
47. How many static pods exist in this cluster in all namespaces?
    kubectl get pods --all-namespaces  # check the pods name including string 'master'
    
48. What is the path of the directory holding the static pod definition files?
    ps -ef | grep kubelet
    
    find this --config=/var/lib/kubelet/config.yaml
    
    grep -i static /var/lib/kubelet/config.yaml
    staticPodPath: /etc/kubernetes/manifests

49. Check image for kube api
     cd /etc/kubernetes/manifests
    grep -i image kube-apiserver.yaml
    
    
 50. Create a static pod named static-busybox that uses the busybox image and the command sleep 1000   
     cd /etc/kubernetes/manifests
     kubectl run static-busybox --image=busybox --command sleep 1000 --restart=Never --dry-run=client -o yaml > busybox.yaml
     
     Then the static pod will be created automatically
     
     
 51. We just created a new static pod named static-greenbox. Find it and delete it.
     Static pod is per node, should ssh to the corresponding node to delete.
     
root@controlplane:/etc/kubernetes/manifests# kubectl get node node01 -o wide
NAME     STATUS   ROLES    AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME
node01   Ready    <none>   39m   v1.20.0   10.11.35.9    <none>        Ubuntu 18.04.5 LTS   5.4.0-1065-gcp   docker://19.3.0
root@controlplane:/etc/kubernetes/manifests# ssh 10.11.35.9



root@node01:~# ps -ef | grep kubelet | grep config
root     18445     1  0 07:00 ?        00:00:08 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.2
root@node01:~# grep -i static /var/lib/kubelet/config.yaml
staticPodPath: /etc/just-to-mess-with-you
root@node01:~# cd /etc/just-to-mess-with-you
root@node01:/etc/just-to-mess-with-you# ls
greenbox.yaml


rm -rf greenbox.yaml


pod will be deleted automatically
     
/************************************ Practice with Multiple Schedulers ************************************/ 
52. What the name ofthe pod that deploys the default kubernetes scheduler in this env?
    kubectl -n kube-system get pods
    
53. Deploy an additional scheduler to the cluster following the given specification. Use the manifest file used by kubeadm tool. Use a different port than the one used by the current one.    
Namespace: kube-system
Name: my-scheduler
Status: Running
Custom Scheduler Name

    root@controlplane:/etc/kubernetes/manifests# cp kube-scheduler.yaml /root/my-scheduler.yaml
    
    vi my-scheduler.yaml and have below updates:
        - --leader-elect=false
    - --scheduler-name=my-scheduler
    
    
    kubectl create -f my-scheduler.yam
    
    
54. A POD definition file is given. Use it to create a POD with the new custom scheduler.
    File is located at /root/nginx-pod.yaml
    
Name: nginx
Uses custom scheduler
Status: Running

     Add schedulerName: my-scheduler under 'spec' of nginx-pod.yaml
     
     kubectl create -f nginx-pod.yaml
    
    
/************************************ Practice with Rolling Update ************************************/ 
55. Let us try that. Upgrade the application by setting the image on the deployment to kodekloud/webapp-color:v2 Do not delete and re-create the deployment. Only set the new image name for the existing deployment.
Deployment Name: frontend
Deployment Image: kodekloud/webapp-color:v2

    kubectl edit deployments.apps frontend   //update image name to v2
    
 
 56. Up to how many PODs can be down for upgrade at a time
     kubectl describe deployments.apps frontend
     
     then we can see that, RollingUpdateStrategy:  25% max unavailable, 25% max surge
     
     25% * 4 = 1 pod max can be down.
     
     
     
/************************************ Practice with Commands & Arguments ************************************/ 
57. Create a pod with the ubuntu image to run a container to sleep for 5000 seconds. Modify the file ubuntu-sleeper-2.yaml.
Pod Name: ubuntu-sleeper-2
Command: sleep 5000

     controlplane ~ ✖ cat ubuntu-sleeper-2.yaml 
apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-2 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command: ["sleep", "5000"]
    
    
    kubectl apply -f ubuntu-sleeper-2.yaml
    
58.  What command is run at container startup?
     controlplane ~/webapp-color-2 ➜  cat *
FROM python:3.6-alpine

RUN pip install flask

COPY . /opt/

EXPOSE 8080

WORKDIR /opt

ENTRYPOINT ["python", "app.py"]

CMD ["--color", "red"]
apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["--color","green"]   # only command here, not argument
    // command: ["python", "app.py"]
    // args: ["--color", "pink"]     # answer should be python app.py --color pink
    
    
    --color green
    
 
 59. Create a pod with the given specifications. By default it displays a blue background. Set the given command line arguments to change it to green
     Pod Name: webapp-green
Image: kodekloud/webapp-color
Command line arguments: --color=green

     apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    args: ["--color", "green"]
    
    
    kubectl apply -f pod.yaml

/************************************ Practice with Environment Variables ************************************/ 
60. Update the environment variable on the POD to display a green background. Note: Delete and recreate the POD. Only make the necessary changes. Do not modify the name of the Pod.
Pod Name: webapp-color
Label Name: webapp-color
Env: APP_COLOR=green

    kubectl get pod webapp-color -o yaml > pod.yaml
    
    kubectl delete pod webapp-color
    
    vi pod.yaml -> change environment value to 'green'
    
    kubectl apply -f pod.yaml
    
    
    
61. Create a new ConfigMap for the webapp-color POD. Use the spec given below.
ConfigName Name: webapp-config-map
Data: APP_COLOR=darkblue

    kubectl create configmap webapp-config-map --from-literal=APP_COLOR=darkblue
    
62. Update the environment variable on the POD to use the newly created ConfigMap
    Pod Name: webapp-color
EnvFrom: webapp-config-map

    kubectl delete pod webapp-color
    
    kubectl explain pods --recursive | grep envFrom -A3
    
    replace environment as below:
     containers:
  - envFrom:       
      - configMapRef:       
               name: webapp-config-map
               
               
     kubectl apply -f pod.yaml
    
/************************************ Practice with Secret ************************************/ 
63. How many secrets are defined in the default-token secret?
    kubectl describe secrets default-token
    
    then to check data section.
    
64. Create a new secret named db-secret with the data given below.
    Secret Name: db-secret
Secret 1: DB_Host=sql01
Secret 2: DB_User=root
Secret 3: DB_Password=password123

     kubectl create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123
     
 65. Configure webapp-pod to load environment variables from the newly created secret.
 Pod name: webapp-pod
Image name: kodekloud/simple-webapp-mysql
Env From: Secret=db-secret

kubectl get pod webapp-pod -o yaml > web.yaml

vi web.yaml
   envFrom: 
      - secretRef:          
               name: db-secret    
               
    kubectl apply -f web.yaml
    
/************************************ Practice with Multi Container Pod ************************************/ 
66. Create a multi-container pod with 2 containers. Use the spec given below.
If the pod goes into the crashloopbackoff then add sleep 1000 in the lemon container.

Name: yellow
Container 1 Name: lemon
Container 1 Image: busybox
Container 2 Name: gold
Container 2 Image: redis

     kubectl run yellow --image=busybox --restart=Never --dry-run=client -o yaml > pod.yaml
     
spec:
  containers:
  - image: busybox
    name: lemon
  - image: redis
    name: gold
    
     
    kubectl apply -f pod.yaml
    
67. Edit the pod to add a sidecar container to send logs to Elastic Search. Mount the log volume to the sidecar container.
Name: app
Container Name: sidecar
Container Image: kodekloud/filebeat-configured
Volume Mount: log-volume
Mount Path: /var/log/event-simulator/
Existing Container Name: app
Existing Container Image: kodekloud/event-simulator

    kubectl -n elastic-stack get pod app -o yaml > app.yaml
    
    spec:
  containers:
  - image: kodekloud/event-simulator
    imagePullPolicy: Always
    name: app
    volumeMounts:
    - mountPath: /log
      name: log-volume
  - image: kodekloud/filebeat-configured
    name: sidecar
    volumeMounts:
    - mountPath: /var/log/event-simulator/
      name: log-volume

/************************************ Practice with initContainer ************************************/ 
68. Update the pod red to use an initContainer that uses the busybox image and sleeps for 20 seconds
Pod: red
initContainer Configured Correctly

    kubectl get pod red -o yaml > red.yaml
    spec:
  containers:
  - command:
    - sh
    - -c
    - echo The app is running! && sleep 3600
    image: busybox:1.28
    imagePullPolicy: IfNotPresent
    name: red-container
  initContainers:
  - image: busybox
    name: red-initcontainer
    command: ["sleep", "20"]
    
    
    kubectl apply -f red.yaml
    
 /************************************ Practice with OS Upgrade ************************************/
69. We need to take node01 out for maintenance. Empty the node of all applications and mark it unschedulable.
   kubectl drain node01 --ignore-daemonsets
   
70. The maintenance tasks have been completed. Configure the node node01 to be schedulable again.
   kubectl uncordon node01
   
   
/************************************ Practice with Cluster Upgrade ************************************/
71. How many nodes can host workloads in this cluster? Inspect the applications and taints set on the nodes.
    kubectl describe each node to check taint
	
72. What is the latest stable version available for upgrade? Use the kubeadm tool
    kubeadm upgrade plan
	
73. Upgrade the controlplane components to exact version v1.20.0. Upgrade kubeadm tool (if not already), then the master components, and finally the kubelet. Practice referring to the kubernetes documentation page. Note: While upgrading kubelet, if you hit dependency issue while running the apt-get upgrade kubelet command, use the apt install kubelet=1.20.0-00 command instead
    kubeadm version
	
	apt update
	
	apt install kubeadm=1.20.0-00
	
	kubeadm upgrade apply v1.20.0 //Upgrade cluster
	
	apt install kubelet=1.20.0-00
	
	systemctl restart kubelet 
	
74. Upgrade the worker node to the exact version v1.20.0
    
	On the node01 node, run the command run the following commands:

If you are on the master node, run ssh node01 to go to node01


apt update
This will update the package lists from the software repository.


apt install kubeadm=1.20.0-00
This will install the kubeadm version 1.20


kubeadm upgrade node
This will upgrade the node01 configuration.


apt install kubelet=1.20.0-00 This will update the kubelet with the version 1.20.


You may need to restart kubelet after it has been upgraded.
Run: systemctl restart kubelet


Type exit or enter CTL + d to go back to the controlplane node.


/************************************ Practice with Backup & Restore ************************************/
75. What is the version of ETCD running on the cluster? Check the ETCD Pod or Process
    kubectl logs etcd-controlplane -n kube-system  // check ETCD logs
	
	kubectl describe pod etcd-controlplane -n kube-system   // check ETCD image
	
76. At what address can you reach the ETCD cluster from the controlplane node?

root@controlplane:~# kubectl -n kube-system describe pod etcd-controlplane | grep '\--listen-client-urls'
      --listen-client-urls=https://127.0.0.1:2379,https://10.2.43.11:2379
root@controlplane:~#
	
77. The master nodes in our cluster are planned for a regular maintenance reboot tonight. While we do not anticipate anything to go wrong, we are required to take the necessary backups. Take a snapshot of the ETCD database using the built-in snapshot functionality.
Store the backup file at location /opt/snapshot-pre-boot.db
    
	root@controlplane:~# ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot save /opt/snapshot-pre-boot.db


Snapshot saved at /opt/snapshot-pre-boot.db
root@controlplane:~# 


78. Luckily we took a backup. Restore the original state of the cluster using the backup file.
    root@controlplane:~# ETCDCTL_API=3 etcdctl  --data-dir /var/lib/etcd-from-backup snapshot restore /opt/snapshot-pre-boot.db
	
	vi /etc/kubernetes/manifests/etcd.yaml
	volumes:
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data
	
	With this change, /var/lib/etcd on the container points to /var/lib/etcd-from-backup on the controlplane (which is what we want)
When this file is updated, the ETCD pod is automatically re-created as this is a static pod placed under the /etc/kubernetes/manifests directory.
   
/************************************ Practice with View certificate details ************************************/ 
79. Identify the certificate file used for the kube-api server
    Run the command cat /etc/kubernetes/manifests/kube-apiserver.yaml and look for the line --tls-cert-file
	
80. Identify the Certificate file used to authenticate kube-apiserver as a client to ETCD Server
    Run the command cat /etc/kubernetes/manifests/kube-apiserver.yaml and look for value of etcd-certfile flag.
	
81. Identify the ETCD Server Certificate used to host ETCD server
    Look for cert-file option in the file /etc/kubernetes/manifests/etcd.yaml
	
82. Identify the ETCD Server CA Root Certificate used to serve ETCD Server
ETCD can have its own CA. So this may be a different CA certificate than the one used by kube-api server.
    Look for CA Certificate (trusted-ca-file) in file /etc/kubernetes/manifests/etcd.yaml
	
83. What is the Common Name (CN) configured on the Kube API Server Certificate? OpenSSL Syntax: openssl x509 -in file-path.crt -text -noout
    openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text to check Subject CN
	
84. What is the Common Name (CN) configured on the ETCD Server certificate?
    Run the command openssl x509 -in /etc/kubernetes/pki/etcd/server.crt -text and look for Subject CN.

85. Kubectl suddenly stops responding to your commands. Check it out! Someone recently modified the /etc/kubernetes/manifests/etcd.yaml file
You are asked to investigate and fix the issue. Once you fix the issue wait for sometime for kubectl to respond. Check the logs of the ETCD container.
    The certificate file used here is incorrect. It is set to /etc/kubernetes/pki/etcd/server-certificate.crt which does not exist. As we saw in the previous questions the correct path should be /etc/kubernetes/pki/etcd/server.crt.
	
86. The kube-api server stopped again! Check it out. Inspect the kube-api server logs and identify the root cause and fix the issue.
Run docker ps -a command to identify the kube-api server container. Run docker logs container-id command to view the logs.	

     docker ps -a | grep kube-apiserver
	 docker logs 9e89d623fce2 --tail=2
	 vi /etc/kubernetes/manifests/kube-apiserver.yaml //This indicates an issue with the ETCD CA certificate used by the kube-apiserver. Correct it to use the file /etc/kubernetes/pki/etcd/ca.crt


/************************************ Practice with Certificates API ************************************/ 
87. Create a CertificateSigningRequest object with the name akshay with the contents of the akshay.csr file
---
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: akshay
spec:
  groups:
  - system:authenticated
  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZqQ0NBVDRDQVFBd0VURVBNQTBHQTFVRUF3d0dZV3R6YUdGNU1JSUJJakFOQmdrcWhraUc5dzBCQVFFRgpBQU9DQVE4QU1JSUJDZ0tDQVFFQXY4azZTTE9HVzcrV3JwUUhITnI2TGFROTJhVmQ1blNLajR6UEhsNUlJYVdlCmJ4RU9JYkNmRkhKKzlIOE1RaS9hbCswcEkwR2xpYnlmTXozL2lGSWF3eGVXNFA3bDJjK1g0L0lqOXZQVC9jU3UKMDAya2ZvV0xUUkpQbWtKaVVuQTRpSGxZNDdmYkpQZDhIRGFuWHM3bnFoenVvTnZLbWhwL2twZUVvaHd5MFRVMAo5bzdvcjJWb1hWZTVyUnNoMms4dzV2TlVPL3BBdEk4VkRydUhCYzRxaHM3MDI1ZTZTUXFDeHUyOHNhTDh1blJQCkR6V2ZsNVpLcTVpdlJNeFQrcUo0UGpBL2pHV2d6QVliL1hDQXRrRVJyNlMwak9XaEw1Q0ErVU1BQmd5a1c5emQKTmlXbnJZUEdqVWh1WjZBeWJ1VzMxMjRqdlFvbndRRUprNEdoayt2SU53SURBUUFCb0FBd0RRWUpLb1pJaHZjTgpBUUVMQlFBRGdnRUJBQi94dDZ2d2EweWZHZFpKZ1k2ZDRUZEFtN2ZiTHRqUE15OHByTi9WZEdxN25oVDNUUE5zCjEwRFFaVGN6T21hTjVTZmpTaVAvaDRZQzQ0QjhFMll5Szg4Z2lDaUVEWDNlaDFYZnB3bnlJMVBDVE1mYys3cWUKMkJZTGJWSitRY040MDU4YituK24wMy9oVkN4L1VRRFhvc2w4Z2hOaHhGck9zRUtuVExiWHRsK29jQ0RtN3I3UwpUYTFkbWtFWCtWUnFJYXFGSDd1dDJveHgxcHdCdnJEeGUvV2cybXNqdHJZUXJ3eDJmQnErQ2Z1dm1sVS9rME4rCml3MEFjbVJsMy9veTdqR3ptMXdqdTJvNG4zSDNKQ25SbE41SnIyQkZTcFVQU3dCL1lUZ1ZobHVMNmwwRERxS3MKNTdYcEYxcjZWdmJmbTRldkhDNnJCSnNiZmI2ZU1KejZPMUU9Ci0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth
  
  kubectl apply -f akshay.yaml
  
  //Approve CSR
  kubectl certificate approve akshay
  
  //Deny CSR
  kubectl certificate deny agent-smith
  
  //delete CSR
  kubectl delete csr agent-smith
  
/************************************ Practice with kubeConfig ************************************/ 
88. kubeconfig file
    /root/.kube/config
	
89. How many clusters are defined in the default kubeconfig file?
    kubectl config view
	
90. I would like to use the dev-user to access test-cluster-1. Set the current context to the right one so I can do that.
    kubectl config --kubeconfig=/root/my-kube-config use-context research
	kubectl config --kubeconfig=/root/my-kube-config current-context


/************************************ Practice with Role Based Access Control ************************************/ 
91. Inspect the environment and identify the authorization modes configured on the cluster. Check the kube-apiserver settings.	
    kubectl describe pod kube-apiserver-controlplane -n kube-system 
	look for --authorization-mode.

92. How many roles exist in all namespaces together?
    kubectl get roles -A
	
93. What are the resources the kube-proxy role in the kube-system namespace is given access to?
    kubectl describe role kube-proxy -n kube-system
	
94.	Which account is the kube-proxy role assigned to it?
	kubectl describe rolebinding kube-proxy -n kube-system

95. A user dev-user is created. User's details have been added to the kubeconfig file. Inspect the permissions granted to the user. Check if the user can list pods in the default namespace.
Use the --as dev-user option with kubectl to run commands as the dev-user.
    kubectl get pods --as dev-user
	
96. Create the necessary roles and role bindings required for the dev-user to create, list and delete pods in the default namespace.
    kubectl create role developer --namespace=default --verb=list,create,delete --resource=pods
	kubectl create rolebinding dev-user-binding --namespace=default --role=developer --user=dev-user
	
	or yaml file:
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: developer
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list", "create","delete"]

---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dev-user-binding
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io
  
97. The dev-user is trying to get details about the dark-blue-app pod in the blue namespace. Investigate and fix the issue.
    Run the command: kubectl edit role developer -n blue and correct the resourceNames field. You don't have to delete the role.
	
98. Grant the dev-user permissions to create deployments in the blue namespace.
    ---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: blue
  name: deploy-role
rules:
- apiGroups: ["apps", "extensions"]
  resources: ["deployments"]
  verbs: ["create"]

---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dev-user-deploy-binding
  namespace: blue
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: deploy-role
  apiGroup: rbac.authorization.k8s.io
  
/************************************ Practice with Cluster Role and RoleBinding ************************************/ 
99. What user/groups are the cluster-admin role bound to?
The ClusterRoleBinding for the role is with the same name.
    
100. What level of permission does the cluster-admin role grant?
Inspect the cluster-admin role's privileges.
    kubectl describe clusterrole cluster-admin
	
101. A new user michelle joined the team. She will be focusing on the nodes in the cluster. Create the required ClusterRoles and ClusterRoleBindings so she gets access to the nodes.
    ---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: node-admin
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "watch", "list", "create", "delete"]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: michelle-binding
subjects:
- kind: User
  name: michelle
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: node-admin
  apiGroup: rbac.authorization.k8s.io
  
  
102. michelle's responsibilities are growing and now she will be responsible for storage as well. Create the required ClusterRoles and ClusterRoleBindings to allow her access to Storage.
Get the API groups and resource names from command kubectl api-resources. Use the given spec:
ClusterRole: storage-admin
Resource: persistentvolumes
Resource: storageclasses
ClusterRoleBinding: michelle-storage-admin
ClusterRoleBinding Subject: michelle
ClusterRoleBinding Role: storage-admin

---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: storage-admin
rules:
- apiGroups: [""]
  resources: ["persistentvolumes"]
  verbs: ["get", "watch", "list", "create", "delete"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses"]
  verbs: ["get", "watch", "list", "create", "delete"]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: michelle-storage-admin
subjects:
- kind: User
  name: michelle
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: storage-admin
  apiGroup: rbac.authorization.k8s.io
  

/************************************ Practice with Service Account ************************************/ 
103. The application needs a ServiceAccount with the Right permissions to be created to authenticate to Kubernetes. The 'default' ServiceAccount has limited access. Create a new ServiceAccount named 'dashboard-sa'.
     kubectl create serviceaccount dashboard-sa
	 
104. You shouldn't have to copy and paste the token each time. The Dashboard application is programmed to read token from the secret mount location. However currently, the 'default' service account is mounted. Update the deployment to use the newly created ServiceAccount
     Deployment name: web-dashboard
Service Account: dashboard-sa
Deployment Ready

    apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-dashboard
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      name: web-dashboard
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        name: web-dashboard
    spec:
      serviceAccountName: dashboard-sa
      containers:
      - image: gcr.io/kodekloud/customimage/my-kubernetes-dashboard
        imagePullPolicy: Always
        name: web-dashboard
        ports:
        - containerPort: 8080
          protocol: TCP   	 

/************************************ Practice with Image Security ************************************/ 
105. What is the secret type we choose for the docker registry?
     kubectl create secret --help
	 
106. We decided to use a modified version of the application from an internal private registry. Update the image of the deployment to use a new image from myprivateregistry.com:5000
     Use the kubectl edit deployment command to edit the image name to myprivateregistry.com:5000/nginx:alpine.
	 
107. Create a secret object with the credentials required to access the registry.
Name: private-reg-cred
Username: dock_user
Password: dock_password
Server: myprivateregistry.com:5000
Email: dock_user@myprivateregistry.com

Secret: private-reg-cred
Secret Type: docker-registry
Secret Data

    Run the command: kubectl create secret docker-registry private-reg-cred --docker-username=dock_user --docker-password=dock_password --docker-server=myprivateregistry.com:5000 --docker-email=dock_user@myprivateregistry.com
	
108. Configure the deployment to use credentials from the new secret to pull images from the private registry
     Edit deployment using kubectl edit deploy web command and add imagePullSecrets section. Use private-reg-cred.
	 
	 imagePullSecrets: 
     - name: private-reg-cred

/************************************ Practice with Security Context ************************************/ 
109. What is the user used to execute the sleep process within the ubuntu-sleeper pod?
    Run the command: kubectl exec ubuntu-sleeper -- whoami and check the user that is running the container.
	
110. Edit the pod ubuntu-sleeper to run the sleep process with user ID 1010.
	kubectl delete po ubuntu-sleeper
	
	---
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper
  namespace: default
spec:
  securityContext:
    runAsUser: 1010
  containers:
  - command:
    - sleep
    - "4800"
    image: ubuntu
    name: ubuntu-sleeper
	
	kubectl apply -f u.yaml
	
111. Update pod ubuntu-sleeper to run as Root user and with the SYS_TIME capability.
  
  Pod Name: ubuntu-sleeper
Image Name: ubuntu
SecurityContext: Capability SYS_TIME

    kubectl delete po ubuntu-sleeper
	
	---
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper
  namespace: default
spec:
  containers:
  - command:
    - sleep
    - "4800"
    image: ubuntu
    name: ubuntu-sleeper
    securityContext:
      capabilities:
        add: ["SYS_TIME"]   // or add: ["SYS_TIME", "NET_ADMIN"] update the pod to also make use of the NET_ADMIN capability
		
/************************************ Practice with Network Policies ************************************/ 
112. How many network policies do you see in the environment?
     kubectl get networkpolicy

113. Create a network policy to allow traffic from the Internal application only to the payroll-service and db-service.
     Policy Name: internal-policy
Policy Type: Egress
Egress Allow: payroll
Payroll Port: 8080
Egress Allow: mysql
MySQL Port: 3306

     apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
  - Egress
  - Ingress
  ingress:
    - {}
  egress:
  - to:
    - podSelector:
        matchLabels:
          name: mysql
    ports:
    - protocol: TCP
      port: 3306

  - to:
    - podSelector:
        matchLabels:
          name: payroll
    ports:
    - protocol: TCP
      port: 8080

  - ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP    


/************************************ Practice with Persistent Volumn Claims ************************************/
pod -> pvc -> pv
So if we want to delete pvc, please delete pod which is binding to this PVC firstly.

114. Configure a volume to store these logs at /var/log/webapp on the host.
Name: webapp
Image Name: kodekloud/event-simulator
Volume HostPath: /var/log/webapp
Volume Mount: /log

apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
    env:
    - name: LOG_HANDLERS
      value: file
    volumeMounts:
    - mountPath: /log
      name: log-volume

  volumes:
  - name: log-volume
    hostPath:
      # directory location on host
      path: /var/log/webapp
      # this field is optional
      type: Directory
	 
	 
115. Create a Persistent Volume with the given specification.
Volume Name: pv-log
Storage: 100Mi
Access Modes: ReadWriteMany
Host Path: /pv/log
Reclaim Policy: Retain


    apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-log
spec:
  persistentVolumeReclaimPolicy: Retain
  accessModes:
    - ReadWriteMany
  capacity:
    storage: 100Mi
  hostPath:
    path: /pv/log
	
	
116. Let us claim some of that storage for our application. Create a Persistent Volume Claim with the given specification.
Volume Name: claim-log-1
Storage Request: 50Mi
Access Modes: ReadWriteOnce

kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: claim-log-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Mi
	  
117. Update the webapp pod to use the persistent volume claim as its storage.
Replace hostPath configured earlier with the newly created PersistentVolumeClaim.

Name: webapp
Image Name: kodekloud/event-simulator
Volume: PersistentVolumeClaim=claim-log-1
Volume Mount: /log

apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
    env:
    - name: LOG_HANDLERS
      value: file
    volumeMounts:
    - mountPath: /log
      name: log-volume

  volumes:
  - name: log-volume
    persistentVolumeClaim:
      claimName: claim-log-1
	  
	  
/************************************ Practice with Storage Class ************************************/	
118. Create a new pod called nginx with the image nginx:alpine. 
The Pod should make use of the PVC local-pvc and mount the volume at the path /var/www/html.
The PV local-pv should in a bound state.  
	  
	  ---
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:alpine
    volumeMounts:
      - name: local-persistent-storage
        mountPath: /var/www/html
  volumes:
    - name: local-persistent-storage
      persistentVolumeClaim:
        claimName: local-pvc
		
119. Create a new Storage Class called delayed-volume-sc that makes use of the below specs:
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer

---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: delayed-volume-sc
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer


/************************************ Practice with Explore Kubernetes Environment ************************************/	
120. What is the Internal IP address of the controlplane node in this cluster?
     kubectl get node controlplane -o wide    then check INTERNAL_IP
	 
121. What is the network interface configured for cluster connectivity on the master node?
    root@controlplane:~# ip addr | grep 10.29.230.9
    inet 10.29.230.9/24 brd 10.29.230.255 scope global eth0
	
	ip link show eth0  to check MAC address

122. What is the MAC address assigned to node01?
     arp node01     //run from master node
	 
	 check default route -> ip route show default
	 
123. What is the port the kube-scheduler is listening on in the controlplane node?
     netstat -nplt | grep scheduler

/************************************ Practice with CNI weave & Deploy Network Solution ************************************/
124. Inspect the kubelet service and identify the network plugin configured for Kubernetes.
     ps -aux | grep kubelet   and look at the configured --network-plugin flag
	 
	 Identify which of the below plugins is not available in the list of available CNI plugins on this host?
	 ls /opt/cni/bin
	 
	 What is the CNI plugin configured to be used on this kubernetes cluster?
	 ls /etc/cni/net.d/
	 
	 What binary executable file will be run by kubelet after a container and its associated namespace are created.
	 Look at the type field in file /etc/cni/net.d/10-flannel.conflist
	 
125. Deploy weave-net networking solution to the cluster.
Replace the default IP address and subnet of weave-net to the 10.50.0.0/16.
     check host ip: ip a | grep eth0
	 
	 kubectl get po -n kube-system 
	 
	 kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')&env.IPALLOC_RANGE=10.50.0.0/16"
    
/************************************ Practice with Networking weave ************************************/

126. What is the Networking Solution used by this cluster?
     Check the config file located at /etc/cni/net.d/
	 
	 How many weave agents/peers are deployed in this cluster?
	 kubectl get pods -n kube-system | grep weave | wc -l
	 
	 What is the POD IP address range configured by weave?
	 ip addr show weave
	 
	 
127. What is the default gateway configured on the PODs scheduled on node01?
Try scheduling a pod on node01 and check ip route output

    kubectl run busybox --image=busybox --command sleep 1000 --dry-run=client -o yaml > pod.yaml
	

spec:
  nodeName: node01
  containers:

    kubectl apply -f pod.yaml
	
	kubectl exec -it busybox -- sh   (it means interactive)
	
	/ # ip route
default via 10.50.192.0 dev eth0 
10.50.0.0/16 dev eth0 scope link  src 10.50.192.1 

/************************************ Practice with Service Networking ************************************/
128. What network range are the nodes in the cluster part of?
     ip addr | grep eth0
	 ipcalc 10.27.54.3/24
	 
	 What is the range of IP addresses configured for PODs on this cluster?
	 kubectl logs <weave-pod-name> weave -n kube-system and look for ipalloc-range
	 
	 What is the IP Range configured for the services within the cluster?
	 cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep cluster-ip-range
	 
	 What type of proxy is the kube-proxy configured to use?
	 kubectl logs <kube-proxy-pod-name> -n kube-system
	 
	 How does this Kubernetes cluster ensure that a kube-proxy pod runs on all nodes in the cluster?
	 kubectl get daemonsets -n kube-system
	 
	 
	 
	 
/************************************ Practice with Core DNS ************************************/
129. Identify the DNS solution implemented in this cluster.
     kubectl get pods -n kube-system
	 
	 What is the IP of the CoreDNS server that should be configured on PODs to resolve services?
	 kubectl get service -n kube-system
	 
	 Where is the configuration file located for configuring the CoreDNS service?
	 kubectl -n kube-system describe deployments.apps coredns | grep -A2 Args | grep Corefile
	 
	 
	 What is the root domain/zone configured for this kubernetes cluster?
	 kubectl describe configmap coredns -n kube-system    and look for the entry after kubernetes.
	 
	 
	 From the hr pod nslookup the mysql service and redirect the output to a file /root/CKA/nslookup.out
	 kubectl exec -it hr -- nslookup mysql.payroll > /root/CKA/nslookup.out
	 

/************************************ Practice with Ingress Networking ************************************/
130. Which namespace is the Ingress Controller deployed in?
     Use the command kubectl get all -A and identify the namespace of Ingress Controller.
	 
	 
	 If the requirement does not match any of the configured paths what service are the requests forwarded to?
	 kubectl describe ingress --namespace app-space and look at the Default backend field.
	 
	 
131. You are requested to add a new path to your ingress to make the food delivery application available to your customers.
Make the new application available at /eat.
    - backend:
          service:
            name: food-service
            port:
              number: 8080
        path: /eat
        pathType: Prefix
		
		
132. You are requested to make the new application available at /pay.
    Ingress Created
Path: /pay
Configure correct backend service
Configure correct backend port



---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress
  namespace: critical-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - path: /pay
        pathType: Prefix
        backend:
          service:
           name: pay-service
           port:
            number: 8282
			
			
134. Let us now deploy an Ingress Controller. First, create a namespace called ingress-space.
     kubectl create namespace ingress-space
	 
	 
	 
	 The NGINX Ingress Controller requires a ConfigMap object. Create a ConfigMap object in the ingress-space.
Use the spec given below. No data needs to be configured in the ConfigMap.
     kubectl create configmap nginx-configuration --namespace ingress-space
	 
	 The NGINX Ingress Controller requires a ServiceAccount. Create a ServiceAccount in the ingress-space namespace.
	 kubectl create serviceaccount ingress-serviceaccount --namespace ingress-space 
	 
	 
135. Let us now create a service to make Ingress available to external users.
Create a service following the given specs.
     Name: ingress
Type: NodePort
Port: 80
TargetPort: 80
NodePort: 30080
Namespace: ingress-space
Use the right selector


     kubectl expose deployment ingress-controller -n ingress-space --type=NodePort --port=80 --name=ingress --dry-run=client -o yaml > ingress.yam
	 
	 and manually add the given node port and namespace
	 
	 
	 apiVersion: v1
kind: Service
metadata:
  name: ingress
  namespace: ingress-space
spec:
  type: NodePort
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30080
    name: http
  - port: 443
    targetPort: 443
    protocol: TCP
    name: https
  selector:
    name: nginx-ingress
	
	
	
136. Create the ingress resource to make the applications available at /wear and /watch on the Ingress service.
Create the ingress in the app-space namespace.
Ingress Created
Path: /wear
Path: /watch
Configure correct backend service for /wear
Configure correct backend service for /watch
Configure correct backend port for /wear service
Configure correct backend port for /watch service



---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear-watch
  namespace: app-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - path: /wear
        pathType: Prefix
        backend:
          service:
           name: wear-service
           port: 
            number: 8080
      - path: /watch
        pathType: Prefix
        backend:
          service:
           name: video-service
           port:
            number: 8080
			
/************************************ Practice with Cluster Install Using kubeadm************************************/
137. Install the kubeadm and kubelet packages on the controlplane and node01. Use the exact version of 1.21.0-00 for both.
     Refer to https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
	 
	 Below steps are performed on both nodes:
set net.bridge.bridge-nf-call-iptables to 1
	 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sudo sysctl --system

Install kubeadm, kubectl and kubelet on all nodes
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl

sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg

echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update
sudo apt-get install -y kubelet=1.21.0-00 kubeadm=1.21.0-00 kubectl=1.21.0-00
sudo apt-mark hold kubelet kubeadm kubectl


138. Initialize Control Plane Node (Master Node). Use the following options:
apiserver-advertise-address - Use the IP address allocated to eth0 on the controlplane node
apiserver-cert-extra-sans - Set it to controlplane
pod-network-cidr - Set to 10.244.0.0/16
Once done, set up the default kubeconfig file and wait for node to be part of the cluster.

root@controlplane:~# ifconfig eth0
eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1450
        inet 10.7.159.3  netmask 255.255.255.0  broadcast 10.7.159.255
        ether 02:42:0a:07:9f:03  txqueuelen 0  (Ethernet)
        RX packets 4792  bytes 604157 (604.1 KB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 4490  bytes 1796591 (1.7 MB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
		
		

kubeadm init --apiserver-cert-extra-sans=controlplane --apiserver-advertise-address 10.7.159.3 --pod-network-cidr=10.244.0.0/16

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
  
  
  
  Generate a kubeadm join token
  kubeadm token create --print-join-command
  
  
ssh node01
kubeadm join 10.7.159.3:6443 --token 0mfdxk.trhovr11wo2uxbzm --discovery-token-ca-cert-hash sha256:2ebaae81a57d08516e5d04f7b4a617bc1e35bd1bd06d62518045fc07b7e7694c 


Install a Network Plugin. As a default, we will go with flannel:
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml


/************************************ Practice with TroubleShooting************************************/
The cluster is broken again. We tried deploying an application but it's not working. Troubleshoot and fix the issue.
root@controlplane:/etc/systemd/system/kubelet.service.d# cd
root@controlplane:~# vi /var/lib/kubelet/config.yaml
root@controlplane:~# cd /etc/kubernetes/manifests
root@controlplane:/etc/kubernetes/manifests# ls
etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml
root@controlplane:/etc/kubernetes/manifests# vi kube-scheduler.yaml 


Scale the deployment app to 2 pods.
kubectl scale deploy app --replicas=2


/************************************ Practice with Json Path************************************/

Get the list of nodes in JSON format and store it in a file at /opt/outputs/nodes.json.
kubectl get nodes -o json > /opt/outputs/nodes.json

Use JSON PATH query to fetch node names and store them in /opt/outputs/node_names.txt.
kubectl get nodes -o=jsonpath='{.items[*].metadata.name}' > /opt/outputs/node_names.txt


Use JSON PATH query to retrieve the osImages of all the nodes and store it in a file /opt/outputs/nodes_os.txt.
The osImages are under the nodeInfo section under status of each node.
kubectl get nodes -o jsonpath='{.items[*].status.nodeInfo.osImage}' > /opt/outputs/nodes_os.txt


 kube-config file is present at /root/my-kube-config. Get the user names from it and store it in a file /opt/outputs/users.txt.
Use the command kubectl config view --kubeconfig=/root/my-kube-config to view the custom kube-config
kubectl config view --kubeconfig=my-kube-config -o jsonpath="{.users[*].name}" > /opt/outputs/users.txt


A set of Persistent Volumes are available. Sort them based on their capacity and store the result in the file /opt/outputs/storage-capacity-sorted.txt.
kubectl get pv --sort-by=.spec.capacity.storage > /opt/outputs/storage-capacity-sorted.txt


That was good, but we don't need all the extra details. Retrieve just the first 2 columns of output and store it in /opt/outputs/pv-and-capacity-sorted.txt.
The columns should be named NAME and CAPACITY. Use the custom-columns option and remember, it should still be sorted as in the previous question.
kubectl get pv --sort-by=.spec.capacity.storage -o=custom-columns=NAME:.metadata.name,CAPACITY:.spec.capacity.storage > /opt/outputs/pv-and-capacity-sorted.txt


Use a JSON PATH query to identify the context configured for the aws-user in the my-kube-config context file and store the result in /opt/outputs/aws-context-name
kubectl config view --kubeconfig=my-kube-config -o jsonpath="{.contexts[?(@.context.user=='aws-user')].name}" > /opt/outputs/aws-context-name
