# This file records the kubernetes commands in CKA training course.

/************************************ Practice with pods ************************************/

1. Get running pods numbers
   kubectl get pods

2. Create a new pod with the nginx image
   kubectl run nginx --image=nginx      // kubectl run POD_NAME --image=POD_IMAGE_NAME
   
   kubectl run nginx --image=nginx --dry-run=client -o yaml > pod.yaml  // one can dry run to check pod details in yaml file
   kubectl apply -f pod.yaml
   
3. What is the image used to create the new pods?
   kubectl describe pod POD_NAMES | grep -i image
   
4. Which nodes are these pods placed on?
   kubectl get pods -o wide    // There is a column named 'node'
   
5. How many containers are part of the pod 'webapp'?
   kubectl get pods webapp
   
6. What is the state of container 'agentx' in the pod 'webapp'?
   kubectl describe pod webapp  // then to check state under each container
   
7. Delete pod webapp
   kubectl delete pod webapp
   
8. Change the image name for a pod
   kubectl edit pod nginx        // find the image name and edit it.


/************************************ Practice with ReplicaSets ************************************/

9. Get replicasets
   kubectl get replicasets.apps
   
controlplane ~ ✖ kubectl get replicasets.apps
NAME              DESIRED   CURRENT   READY   AGE
new-replica-set   4         4         0       36s

We can see that the DESIRED pods numbers in the replica. Replica will always make sure the DESIRED number of pods running. How many are deleted, then how many will be created automatically.

10. Get the image used to create pods in the replica.
    kubectl describe replicasets.apps RELICA_NAME
    
11. Correct replica apiVersion:
    controlplane ~ ➜  kubectl apply -f replicaset-definition-1.yaml 
error: unable to recognize "replicaset-definition-1.yaml": no matches for kind "ReplicaSet" in version "v1"

   -> apps/v1
   
12. Delete ReplicaSet
    kubectl delete replicasets.apps REPLICA_NAME
    
13. Scale (up/down) replica
    kubectl scale replicaset --replicas=5 REPLICA_SET_NAME    // up from 4 to 5
    kubectl scale replicaset --replicas=2 REPLICA_SET_NAME    // dwon from 5 to 2



/************************************ Practice with Deployment ************************************/


14. How many deployments
    kubectl get deployments.apps
    
15. Create deployment:
    Name: httpd-frontend;
Replicas: 3;
Image: httpd:2.4-alpine


    kubectl create deployment httpd-frontend --image=httpd:2.4-alpine
    
    kubectl scale deployment --replicas=3 httpd-frontend
    
    

/************************************ Practice with Namespace ************************************/

16.  How many Namespace
     kubectl get ns --no-headers | wc -l
     
17. How many pods exist in the namespace 'research'?
    kubectl get pods -n research --no-headers
    
18. Create a POD in the 'finance' namespace. (Name: redis, Image Name: redis)
    1) kubectl run redis --image=redis --dry-run=client -o yaml > pod.yaml
    2) vi pod.yaml
       apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: redis
  name: redis
  namespace: finance    #add this line 
spec:
  containers:
  - image: redis
    name: redis
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

    3) kubectl apply -f pod.yaml
    
19. Which namespace has the 'blue' pod in it?
    kubectl get pods --all-namespaces | grep blue
    
 
 /************************************ Practice with Service ************************************/
 
 20. How many Services exist on the system?
     kubectl get services
     
 21. What is the targetPort configured on the kubernetes service?
     controlplane ~ ✖ kubectl describe service kubernetes
Name:              kubernetes
Namespace:         default
Labels:            component=apiserver
                   provider=kubernetes
Annotations:       <none>
Selector:          <none>
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.43.0.1
IPs:               10.43.0.1
Port:              https  443/TCP
TargetPort:        6443/TCP                # Answer is 6443
Endpoints:         10.6.235.3:6443
Session Affinity:  None
Events:            <none>

22. How many Deployments exist on the system now?
    kubectl get deployments.apps
    
23. What is the image used to create the pods in the deployment?
    kubectl describe deployments.apps simple-webapp-deployment
    
24. Create a new service to access the web application using the service-definition-1.yaml file
Name: webapp-service
Type: NodePort
targetPort: 8080
port: 8080
nodePort: 30080
selector: simple-webapp

    kubectl expose deployment simple-webapp-deployment --name=webapp-service --target-port=8080 --type=NodePort --port=8080 --dry-run=client -o yaml > svc.yaml
    kubectl apply -f svc.yaml


 /************************************ Practice with Imperative Commands ************************************/
 
 25. Deploy a pod named nginx-pod using the nginx:alpine image. Use imperative commands only.
     kubectl run nginx-pod --image=nginx:alpine
     
     Deploy a redis pod using the redis:alpine image with the labels set to tier=db.
     kubectl run redis --image=redis:alpine --labels=tier=db
         
 26. Create a service redis-service to expose the redis application within the cluster on port 6379.
     kubectl expose pod redis --name redis-service --port 6379 --target-port 6379
     
 27. Create a deployment named webapp using the image kodekloud/webapp-color with 3 replicas.
     controlplane ~ ➜  kubectl create deployment webapp --image=kodekloud/webapp-color
     deployment.apps/webapp created

     controlplane ~ ➜  kubectl scale deployment --replicas=3 webapp
     deployment.apps/webapp scaled
     
28.  Create a new pod called custom-nginx using the nginx image and expose it on container port 8080.
     kubectl run custom-nginx --image=nginx --port 8080  
     
29. Create a new deployment called redis-deploy in the dev-ns namespace with the redis image. It should have 2 replicas.
     kubectl create deployment redis-deploy --image=redis --namespace=dev-ns --dry-run=client -o yaml > redis.yaml
     kubectl apply -f redis.yaml
     
30. Create a pod called httpd using the image httpd:alpine in the default namespace. Next, create a service of type ClusterIP by the same name (httpd). The target port for the service should be 80.
    kubectl run httpd --image=httpd:alpine --port 80 --expose --dry-run=client -o yaml
    
 
/************************************ Practice with Manually schedule ************************************/
31. Manually schedule the pod on node01. Delete and recreate the POD if necessary.
    kubectl delete pod nginx
    
    root@controlplane:~# vi nginx.yaml 
root@controlplane:~# cat nginx.yaml 
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  nodeName: node01     #add nodeName to pod yaml file
  containers:
  -  image: nginx
     name: nginx
     
     
     kubectl apply -f nginx.yaml
     
     
 
 /************************************ Practice with Labels and Selectors ************************************/    
 32. We have deployed a number of PODs. They are labelled with tier, env and bu. How many PODs exist in the dev environment?
     kubectl get pods -l env=dev
     
     How many PODs are in the finance business unit (bu)?
     kubectl get pods -l bu=finance
     
 33. How many objects are in the prod environment including PODs, ReplicaSets and any other objects?
     kubectl get all -l env=prod --no-headers | wc -l
     
     Identify the POD which is part of the prod environment, the finance BU and of frontend tier?
     kubectl get pods -l env=prod,bu=finance,tier=frontend
     
     
/************************************ Practice with Taint and Tolerance ************************************/    
34. Create a taint on node01 with key of spray, value of mortein and effect of NoSchedule
    kubectl taint node node01 spray=mortein:NoSchedule
   
35. Create another pod named bee with the nginx image, which has a toleration set to the taint mortein.
    kubectl run bee --image=nginx --restart=Never --dry-run=client -o yaml > bee.yaml
       
       cat bee.yaml
       apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: bee
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
 # Add below content
  tolerations:
          - effect: NoSchedule
            key: spray
            operator: Equal
            value: mortein
          
          kubectl apply -f bee.yaml
          
36. Remove the taint on controlplane, which currently has the taint effect of NoSchedule.
    kubectl taint node controlplane node-role.kubernetes.io/master:NoSchedule-    # add - 
    
 /************************************ Practice with Node Affinity ************************************/ 
 37. How many Labels exist on node node01?
     kubectl  get  nodes node01 --show-labels 
     
 38. Apply a label color=blue to node node01
     kubectl label nodes node01 color=blue
  
 39. Create a new deployment named blue with the nginx image and 3 replicas.
     kubectl create deployment blue --image=nginx
     kubectl scale deployment blue --replicas=3
    
 40. Set Node Affinity to the deployment to place the pods on node01 only.
     Name: blue
Replicas: 3
Image: nginx
NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution
Key: color
values: blue


      1) add affinity rule:
        spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: color
                operator: In
                values:
                  - blue
                  
       2) kubectl delete deployments.apps blue
       
       3) kubectl applfy -f blue.yaml
      
      
41. Create a new deployment named red with the nginx image and 2 replicas, and ensure it gets placed on the controlplane node only.
    Use the label - node-role.kubernetes.io/master - set on the controlplane node.      
Name: red
Replicas: 2
Image: nginx
NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution
Key: node-role.kubernetes.io/master
Use the right operator    

         affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/master
                operator: Exists
                
root@controlplane:~# kubectl create deployment red --image=nginx --dry-run=client -o yaml > red.yaml
root@controlplane:~# vi red.yaml 
root@controlplane:~# kubectl apply -f red.yaml 
                  
/************************************ Practice with Resource ************************************/    
42. The elephant pod runs a process that consume 15Mi of memory. Increase the limit of the elephant pod to 20Mi.
    controlplane ~ ➜  kubectl get pod elephant -o yaml > elephant.yaml
    controlplane ~ ➜  vi elephant.yaml    # modify the limit memory to 20Mi
    controlplane ~ ➜  kubectl delete pod elephant 
            pod "elephant" deleted
    controlplane ~ ➜  kubectl apply -f elephant.yaml 
    
    
/************************************ Practice with DaemonSets ************************************/ 
43. How many DaemonSets are created in the cluster in all namespaces?
    kubectl get daemonsets --all-namespaces
   
44. On how many nodes are the pods scheduled by the DaemonSets kube-proxy
    kubectl -n kube-system get pods -o wide | grep proxy

45. What is the image used by the POD deployed by the kube-flannel-ds DaemonSet?
    kubectl -n kube-system describe ds kube-flannel-ds | grep -i image
    
    
46. Deploy a DaemonSet for FluentD Logging. Use the given specifications.
Name: elasticsearch
Namespace: kube-system
Image: k8s.gcr.io/fluentd-elasticsearch:1.20

     kubectl create deployment elasticsearch --image=k8s.gcr.io/fluentd-elasticsearch:1.20 --dry-run=client -o yaml > elastic.yaml
     
     vi elastic.yaml and edit as below:
     apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: elasticsearch
  name: elasticsearch
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: elasticsearch
    spec:
      containers:
      - image: k8s.gcr.io/fluentd-elasticsearch:1.20
        name: fluentd-elasticsearch
        
        
        kubectl apply -f elastic.yaml 
        kubectl -n kube-system get ds elasticsearch

/************************************ Practice with Static pods ************************************/ 
47. How many static pods exist in this cluster in all namespaces?
    kubectl get pods --all-namespaces  # check the pods name including string 'master'
    
48. What is the path of the directory holding the static pod definition files?
    ps -ef | grep kubelet
    
    find this --config=/var/lib/kubelet/config.yaml
    
    grep -i static /var/lib/kubelet/config.yaml
    staticPodPath: /etc/kubernetes/manifests

49. Check image for kube api
     cd /etc/kubernetes/manifests
    grep -i image kube-apiserver.yaml
    
    
 50. Create a static pod named static-busybox that uses the busybox image and the command sleep 1000   
     cd /etc/kubernetes/manifests
     kubectl run static-busybox --image=busybox --command sleep 1000 --restart=Never --dry-run=client -o yaml > busybox.yaml
     
     Then the static pod will be created automatically
     
     
 51. We just created a new static pod named static-greenbox. Find it and delete it.
     Static pod is per node, should ssh to the corresponding node to delete.
     
root@controlplane:/etc/kubernetes/manifests# kubectl get node node01 -o wide
NAME     STATUS   ROLES    AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME
node01   Ready    <none>   39m   v1.20.0   10.11.35.9    <none>        Ubuntu 18.04.5 LTS   5.4.0-1065-gcp   docker://19.3.0
root@controlplane:/etc/kubernetes/manifests# ssh 10.11.35.9



root@node01:~# ps -ef | grep kubelet | grep config
root     18445     1  0 07:00 ?        00:00:08 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.2
root@node01:~# grep -i static /var/lib/kubelet/config.yaml
staticPodPath: /etc/just-to-mess-with-you
root@node01:~# cd /etc/just-to-mess-with-you
root@node01:/etc/just-to-mess-with-you# ls
greenbox.yaml


rm -rf greenbox.yaml


pod will be deleted automatically
     
/************************************ Practice with Multiple Schedulers ************************************/ 
52. What the name ofthe pod that deploys the default kubernetes scheduler in this env?
    kubectl -n kube-system get pods
    
53. Deploy an additional scheduler to the cluster following the given specification. Use the manifest file used by kubeadm tool. Use a different port than the one used by the current one.    
Namespace: kube-system
Name: my-scheduler
Status: Running
Custom Scheduler Name

    root@controlplane:/etc/kubernetes/manifests# cp kube-scheduler.yaml /root/my-scheduler.yaml
    
    vi my-scheduler.yaml and have below updates:
        - --leader-elect=false
    - --scheduler-name=my-scheduler
    
    
    kubectl create -f my-scheduler.yam
    
    
54. A POD definition file is given. Use it to create a POD with the new custom scheduler.
    File is located at /root/nginx-pod.yaml
    
Name: nginx
Uses custom scheduler
Status: Running

     Add schedulerName: my-scheduler under 'spec' of nginx-pod.yaml
     
     kubectl create -f nginx-pod.yaml
    
    
/************************************ Practice with Rolling Update ************************************/ 
55. Let us try that. Upgrade the application by setting the image on the deployment to kodekloud/webapp-color:v2 Do not delete and re-create the deployment. Only set the new image name for the existing deployment.
Deployment Name: frontend
Deployment Image: kodekloud/webapp-color:v2

    kubectl edit deployments.apps frontend   //update image name to v2
    
 
 56. Up to how many PODs can be down for upgrade at a time
     kubectl describe deployments.apps frontend
     
     then we can see that, RollingUpdateStrategy:  25% max unavailable, 25% max surge
     
     25% * 4 = 1 pod max can be down.
     
     
     
